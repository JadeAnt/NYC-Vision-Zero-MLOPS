version: "3.8"

volumes:
  nyc-crashes:

services:

  # ─────────────────────────── 1) Extract stage ────────────────────────────
  extract-data:
    container_name: etl_extract_data
    image: python:3.11-slim
    user: root
    volumes:
      - nyc-crashes:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -euo pipefail
        
        echo 'Resetting dataset directory …'
        rm -rf nyc-crashes
        mkdir -p nyc-crashes
        cd nyc-crashes

        echo 'Downloading NYC collisions CSV …'
        curl -L 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD' \
          -o collisions.csv

        echo 'Extract stage complete. Contents of /data:'
        ls -l /data

  # ─────────────────────────── 2) Transform stage ──────────────────────────
  transform-data:
    container_name: etl_transform_data
    image: python:3.11-slim
    user: root
    depends_on:
      extract-data:
        condition: service_completed_successfully
    volumes:
      - nyc-crashes:/data
    working_dir: /data/nyc-crashes
    command:
      - bash
      - -c
      - |
        set -euo pipefail

        echo 'Installing Python dependencies …'
        pip install --no-cache-dir pandas numpy

        python3 - <<'EOF'
        import os
        import pandas as pd
        from concurrent.futures import ProcessPoolExecutor, as_completed
        import multiprocessing
        from datetime import datetime

        print('Loading CSV …')
        df = pd.read_csv('collisions.csv', low_memory=False)
        print(f'→ {len(df):,} rows, {df.shape[1]:,} columns')

        print('Basic cleaning …')
        final_df = (
            df
            .dropna(subset=['ON STREET NAME', 'CROSS STREET NAME'])
            .assign(
                intersection_id=lambda d:
                    d['ON STREET NAME'].str.strip() + '_' + d['CROSS STREET NAME'].str.strip(),
                CRASH_DATE=lambda d:
                    pd.to_datetime(d['CRASH DATE'], errors='coerce')
            )
            .dropna(subset=['CRASH_DATE'])
            .loc[:, ['CRASH_DATE', 'intersection_id']]
            .reset_index(drop=True)
        )
        print(f'→ final_df has {len(final_df):,} rows')

        print('Grouping by intersection …')
        grouped = (
            final_df
            .groupby('intersection_id')['CRASH_DATE']
            .apply(lambda s: s.sort_values().to_numpy("datetime64[ns]"))
            .to_dict()
        )
        print(f'→ built {len(grouped):,} groups')

        start_date = final_df['CRASH_DATE'].min() + pd.DateOffset(years=5)
        end_date   = final_df['CRASH_DATE'].max() - pd.DateOffset(months=6)
        yearly_dates = pd.date_range(start=start_date, end=end_date, freq='YS')
        OFF_6M, OFF_1Y, OFF_5Y = (
            pd.DateOffset(months=6),
            pd.DateOffset(years=1),
            pd.DateOffset(years=5),
        )

        def process_intersection(item):
            iid, crashes = item
            recs = []
            for ts in yearly_dates:
                past_6m = ((ts - OFF_6M) <= crashes) & (crashes < ts)
                past_1y = ((ts - OFF_1Y) <= crashes) & (crashes < ts)
                past_5y = ((ts - OFF_5Y) <= crashes) & (crashes < ts)
                fut_6m  = (ts <= crashes) & (crashes < ts + OFF_6M)

                cnt_fut = int(fut_6m.sum())
                if cnt_fut == 0:
                    continue

                recs.append({
                    'prediction_time':     ts,
                    'intersection_id':     iid,
                    'accidents_6m':        int(past_6m.sum()),
                    'accidents_1y':        int(past_1y.sum()),
                    'accidents_5y':        int(past_5y.sum()),
                    'future_accidents_6m': cnt_fut,
                })
            return recs

        print('Running parallel processing …')
        results = []
        with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as exe:
            futures = [exe.submit(process_intersection, item) for item in grouped.items()]
            for fut in as_completed(futures):
                results.extend(fut.result())

        processed_df = pd.DataFrame(results)
        print(f'→ processed_df has {len(processed_df):,} rows')

        print('Writing per-year CSVs …')
        processed_df['YEAR'] = processed_df['prediction_time'].dt.year
        for yr, df_y in processed_df.groupby('YEAR'):
            out_dir  = f'year_{yr}'
            os.makedirs(out_dir, exist_ok=True)
            df_y.to_csv(os.path.join(out_dir, f'processed_{yr}.csv'), index=False)
            print(f'→ wrote {len(df_y):,} rows to year_{yr}/')

        os.remove("collisions.csv")
        print('Transform complete.')
        EOF

        echo 'Transform stage complete. Directory listing:'
        ls -l /data/nyc-crashes
        find /data/nyc-crashes -maxdepth 1 -type d -name 'year_*' | sort

  # ─────────────────────────── 3) Load stage ────────────────────────────────
  load-data:
    container_name: etl_load_data
    image: rclone/rclone:latest
    user: root
    depends_on:
      transform-data:
        condition: service_completed_successfully
    volumes:
      - nyc-crashes:/data
      - ${HOME}/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    environment:
      RCLONE_CONTAINER: ${RCLONE_CONTAINER}
    command:
      - /bin/sh
      - -c
      - |
        set -euo pipefail

        if [ -z "${RCLONE_CONTAINER:-}" ]; then
          echo 'ERROR: RCLONE_CONTAINER env var is not set'
          exit 1
        fi

        echo 'Cleaning remote container …'
        rclone delete chi_tacc:${RCLONE_CONTAINER} --rmdirs || true

        echo 'Uploading new data …'
        rclone copy /data/nyc-crashes chi_tacc:${RCLONE_CONTAINER} \
               --progress --transfers=32 --checkers=16 \
               --multi-thread-streams=4 --fast-list

        echo 'Remote listing after load stage:'
        rclone lsd chi_tacc:${RCLONE_CONTAINER}
